import os
import json
import time
from celery import Celery
from pydantic import BaseModel, Field
from typing import Dict, Any, Optional
import requests
import torch  # â—€â—€â—€ [ì¶”ê°€] GPU ì‚¬ìš© ì—¬ë¶€ í™•ì¸ìš©

from .config import settings
from .logger_config import setup_logging
from .pipeline.classification_module import DocumentClassifier  # â—€â—€â—€ ì´ì œ 'ì§„ì§œ' ëª¨ë“ˆ
from .pipeline.ocr_module import OCRModule  # â—€â—€â—€ ì´ì œ 'ìµœì‹  PaddleOCR' ëª¨ë“ˆ
from .pipeline.llm_tasks import (
    get_llm_extraction_task,
    perform_pii_masking,
    perform_summarization
)
from .pipeline.client import SearchClient, VectorClient

# ë¡œê±° ì„¤ì •
log = setup_logging()

# --- Celery ì„¤ì • ---
celery_app = Celery(
    "worker",
    broker=settings.REDIS_BROKER_URL,  # <-- ğŸ’¡ ì´ë ‡ê²Œ ìˆ˜ì •
    backend=settings.REDIS_BROKER_URL, # <-- ğŸ’¡ ì—¬ê¸°ë„ ë˜‘ê°™ì´ ìˆ˜ì •
    include=["app.tasks"],
)
celery_app.conf.task_track_started = True
celery_app.conf.task_serializer = 'json'
celery_app.conf.result_serializer = 'json'
celery_app.conf.accept_content = ['json']

# --- AI ëª¨ë¸ ë° í´ë¼ì´ì–¸íŠ¸ ë¡œë“œ ---
MODEL_PATH = os.getenv("MODEL_PATH", "/usr/src/models/classifier")
log.info(f"Attempting to load DocumentClassifier model from: {MODEL_PATH}")

# [ìˆ˜ì •] ëª¨ë¸ ë¡œë“œë¥¼ ìƒì„±ìì—ì„œ í˜¸ì¶œ
classifier_model = DocumentClassifier(model_path=MODEL_PATH)

# [ìˆ˜ì •] PaddleOCR ëª¨ë“ˆ ì¸ìŠ¤í„´ìŠ¤í™”
try:
    USE_GPU = torch.cuda.is_available()
    log.info(f"GPU Available: {USE_GPU}")
    ocr_module = OCRModule(lang='en', use_gpu=USE_GPU)
except Exception as e:
    log.error(f"Failed to load OCRModule: {e}. OCR tasks will fail.", exc_info=True)
    ocr_module = None

# ë°ì´í„°ë² ì´ìŠ¤ í´ë¼ì´ì–¸íŠ¸
search_client = SearchClient()
vector_client = VectorClient()


# --- ë¬¸ì„œ ì²˜ë¦¬ ì»¨í…ìŠ¤íŠ¸ ---
class DocumentContext(BaseModel):
    job_id: str
    file_path: str
    file_name: str
    file_mime_type: str

    extracted_text: Optional[str] = None
    classification_result: Dict[str, Any] = Field(default_factory=dict)
    structured_data: Dict[str, Any] = Field(default_factory=dict)
    summary: Optional[str] = None
    pii_results: Dict[str, Any] = Field(default_factory=dict)

    start_time: float = Field(default_factory=time.time)
    ocr_time: float = 0.0
    classification_time: float = 0.0
    extraction_time: float = 0.0
    total_time: float = 0.0


# --- íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ (í•¨ìˆ˜) ---

def perform_ocr(context: DocumentContext):
    """1. OCR ìˆ˜í–‰ (ì´ì œ 'ìµœì‹  PaddleOCR' ëª¨ë“ˆ ì‚¬ìš©)"""
    log.info(f"[{context.job_id}] (1/6) Starting OCR...")
    t_start = time.time()

    if ocr_module is None:
        log.error(f"[{context.job_id}] OCRModule is not loaded. Skipping OCR.")
        context.extracted_text = ""
        return

    try:
        # [ìˆ˜ì •] 'ìµœì‹  PaddleOCR' ëª¨ë“ˆì˜ perform_ocr í•¨ìˆ˜ ì‹¤í–‰
        context.extracted_text = ocr_module.perform_ocr(context.file_path)

        context.ocr_time = time.time() - t_start
        log.info(f"[{context.job_id}] (1/6) OCR finished ({context.ocr_time:.2f}s). "
                 f"Extracted {len(context.extracted_text)} chars.")
    except Exception as e:
        log.error(f"[{context.job_id}] OCR task failed: {e}", exc_info=True)
        context.extracted_text = ""


def perform_classification(context: DocumentContext):
    """2. ë¬¸ì„œ ë¶„ë¥˜ (ì´ì œ 'ì§„ì§œ' ëª¨ë“ˆ + 'ë°ëª¨ ëª¨ë“œ' ì‚¬ìš©)"""
    log.info(f"[{context.job_id}] (2/6) Starting Classification...")
    t_start = time.time()

    if not context.extracted_text:
        log.warning(f"[{context.job_id}] No text extracted from OCR. Skipping classification.")
        context.classification_result = {"doc_type": "ocr_failed", "confidence": 0.0}
        return

    try:
        # â—€â—€â—€ [ìˆ˜ì •] 'classify' í•¨ìˆ˜ì— file_name ì¸ì ì „ë‹¬ â—€â—€â—€
        classification_result = classifier_model.classify(
            text=context.extracted_text,
            file_name=context.file_name
        )
        # â—€â—€â—€ [ìˆ˜ì • ì™„ë£Œ] â—€â—€â—€

        context.classification_result = classification_result
        context.classification_time = time.time() - t_start
        log.info(f"[{context.job_id}] (2/6) Classification finished ({context.classification_time:.2f}s). "
                 f"Result: {classification_result}")
    except Exception as e:
        log.error(f"[{context.job_id}] Classification task failed: {e}", exc_info=True)
        context.classification_result = {"doc_type": "classification_failed", "confidence": 0.0}


def perform_llm_extraction(context: DocumentContext):
    """3. LLM ê¸°ë°˜ ì •ë³´ ì¶”ì¶œ (ë¶„ë¥˜ ê²°ê³¼ì— ë”°ë¼)"""
    doc_type = context.classification_result.get("doc_type", "unknown")
    extraction_task = get_llm_extraction_task(doc_type)

    if not extraction_task:
        log.info(f"[{context.job_id}] (3/6) No extraction task defined for doc_type '{doc_type}'. Skipping.")
        return

    log.info(f"[{context.job_id}] (3/6) Starting LLM Extraction for '{doc_type}'...")
    t_start = time.time()

    try:
        context.structured_data = extraction_task(context.extracted_text)
        context.extraction_time = time.time() - t_start
        log.info(f"[{context.job_id}] (3/6) LLM Extraction finished ({context.extraction_time:.2f}s).")
    except Exception as e:
        log.error(f"[{context.job_id}] LLM Extraction task failed: {e}", exc_info=True)
        context.structured_data = {"error": str(e)}


def perform_pii_and_summary(context: DocumentContext):
    """4. (ë³‘ë ¬) PII íƒì§€ ë° ìš”ì•½"""
    log.info(f"[{context.job_id}] (4/6) Starting PII Masking...")
    try:
        context.pii_results = perform_pii_masking(context.extracted_text)
        log.info(f"[{context.job_id}] (4/6) PII Masking finished.")
    except Exception as e:
        log.error(f"[{context.job_id}] PII Masking task failed: {e}", exc_info=True)

    log.info(f"[{context.job_id}] (5/6) Starting Summarization...")
    try:
        context.summary = perform_summarization(context.extracted_text)
        log.info(f"[{context.job_id}] (5/6) Summarization finished.")
    except Exception as e:
        log.error(f"[{context.job_id}] Summarization task failed: {e}", exc_info=True)


def perform_indexing(context: DocumentContext):
    """6. ê²€ìƒ‰ ì—”ì§„ì— ì¸ë±ì‹±"""
    log.info(f"[{context.job_id}] (6/6) Starting Indexing...")

    try:
        # 1. MeiliSearch (Full-Text Search)
        meili_doc = {
            "id": context.job_id,
            "file_name": context.file_name,
            "doc_type": context.classification_result.get("doc_type", "unknown"),
            "content": context.extracted_text,
            "summary": context.summary,
            "created_at": int(context.start_time),
            **context.structured_data
        }
        search_client.index_document(meili_doc)
        log.info(f"[{context.job_id}] (6/6) MeiliSearch indexing finished.")

        # 2. Qdrant (Vector Search)
        vector_client.index_document(
            doc_id=context.job_id,
            text_content=context.extracted_text,
            metadata=meili_doc
        )
        log.info(f"[{context.job_id}] (6/6) Qdrant indexing finished.")

    except Exception as e:
        log.error(f"[{context.job_id}] Indexing task failed: {e}", exc_info=True)


def cleanup_file(file_path: str):
    """íŒŒì´í”„ë¼ì¸ ì™„ë£Œ í›„ ì„ì‹œ íŒŒì¼ ì‚­ì œ"""
    try:
        if os.path.exists(file_path):
            os.remove(file_path)
            log.info(f"Cleaned up temp file: {file_path}")
    except Exception as e:
        log.error(f"Failed to cleanup file {file_path}: {e}")


# --- ë©”ì¸ Celery ì‘ì—… ---

@celery_app.task(name="process_document_pipeline")
def process_document_pipeline(
        job_id: str,
        file_path: str,
        file_name: str,
        file_mime_type: str
):
    """
    íŒŒì¼ ì—…ë¡œë“œ í›„ ì‹¤í–‰ë˜ëŠ” ë©”ì¸ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
    (OCR -> ë¶„ë¥˜ -> ì¶”ì¶œ -> PII/ìš”ì•½ -> ì¸ë±ì‹±)
    """
    log.info(f"[{job_id}] Celery task started for file: {file_name}")

    context = DocumentContext(
        job_id=job_id,
        file_path=file_path,
        file_name=file_name,
        file_mime_type=file_mime_type
    )

    try:
        # --- íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ---
        perform_ocr(context)
        perform_classification(context)
        perform_llm_extraction(context)
        perform_pii_and_summary(context)
        perform_indexing(context)
        # --- íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ ---

        context.total_time = time.time() - context.start_time
        log.info(f"[{job_id}] Pipeline COMPLETED ({context.total_time:.2f}s).")

        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        cleanup_file(file_path)

        # ìµœì¢… ê²°ê³¼ ë°˜í™˜ (Celery Result Backendì— ì €ì¥ë¨)
        return json.loads(context.model_dump_json())

    except Exception as e:
        log.critical(f"[{job_id}] Unhandled error in pipeline: {e}", exc_info=True)
        cleanup_file(file_path)
        error_result = context.model_dump()
        error_result["pipeline_status"] = "FAILED"
        error_result["error"] = str(e)
        return error_result