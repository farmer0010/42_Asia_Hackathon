# D:\42_asia-hackathon\app\worker.py

import os
import json
import time
from celery import Celery
from pydantic import BaseModel, Field
from typing import Dict, Any, Optional
from .config import settings
import requests
import torch
import uuid  # ğŸš¨ Qdrant í¬ì¸íŠ¸ ID ìƒì„±ì„ ìœ„í•´ ì¶”ê°€

from .config import settings
from .logger_config import setup_logging
from .pipeline.classification_module import DocumentClassifier
from .pipeline.ocr_module import OCRModule
from .pipeline.llm_tasks import (
    get_llm_extraction_task,
    perform_pii_masking,
    perform_summarization
)
from .pipeline.client import SearchClient, VectorClient
# ğŸš¨ Qdrant í¬ì¸íŠ¸ ìƒì„±ì„ ìœ„í•´ models ì„í¬íŠ¸
from qdrant_client.http import models as qdrant_models

log = setup_logging()

# --- Celery ì„¤ì • ---
celery_app = Celery(
    "worker",
    broker=settings.CELERY_BROKER_URL,  # ğŸš¨ [ìˆ˜ì • 7]: config.pyì™€ .envì— ë§ê²Œ ìˆ˜ì •
    backend=settings.CELERY_RESULT_BACKEND,  # ğŸš¨ [ìˆ˜ì • 7]: config.pyì™€ .envì— ë§ê²Œ ìˆ˜ì •
    include=["app.tasks"],
)
celery_app.conf.task_track_started = True
celery_app.conf.task_serializer = 'json'
celery_app.conf.result_serializer = 'json'
celery_app.conf.accept_content = ['json']

# --- AI ëª¨ë¸ ë° í´ë¼ì´ì–¸íŠ¸ ë¡œë“œ ---
MODEL_PATH = os.getenv("MODEL_PATH", "/usr/src/models/classifier")
log.info(f"Attempting to load DocumentClassifier model from: {MODEL_PATH}")

classifier_model = DocumentClassifier(model_path=MODEL_PATH)

try:
    USE_GPU = torch.cuda.is_available()
    log.info(f"GPU Available: {USE_GPU}")
    ocr_module = OCRModule(lang='en', use_gpu=USE_GPU)
except Exception as e:
    log.error(f"Failed to load OCRModule: {e}. OCR tasks will fail.", exc_info=True)
    ocr_module = None

search_client = SearchClient()
vector_client = VectorClient()


# --- ë¬¸ì„œ ì²˜ë¦¬ ì»¨í…ìŠ¤íŠ¸ ---
class DocumentContext(BaseModel):
    job_id: str
    file_path: str
    file_name: str
    file_mime_type: str

    extracted_text: Optional[str] = None
    classification_result: Dict[str, Any] = Field(default_factory=dict)
    structured_data: Dict[str, Any] = Field(default_factory=dict)
    summary: Optional[str] = None
    pii_results: Dict[str, Any] = Field(default_factory=dict)

    start_time: float = Field(default_factory=time.time)
    ocr_time: float = 0.0
    classification_time: float = 0.0
    extraction_time: float = 0.0
    total_time: float = 0.0


# --- íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ (í•¨ìˆ˜) ---

def perform_ocr(context: DocumentContext):
    log.info(f"[{context.job_id}] (1/6) Starting OCR...")
    t_start = time.time()
    if ocr_module is None:
        log.error(f"[{context.job_id}] OCRModule is not loaded. Skipping OCR.")
        context.extracted_text = ""
        return
    try:
        context.extracted_text = ocr_module.perform_ocr(context.file_path)
        context.ocr_time = time.time() - t_start
        log.info(
            f"[{context.job_id}] (1/6) OCR finished ({context.ocr_time:.2f}s). Extracted {len(context.extracted_text)} chars.")
    except Exception as e:
        log.error(f"[{context.job_id}] OCR task failed: {e}", exc_info=True)
        context.extracted_text = ""


def perform_classification(context: DocumentContext):
    log.info(f"[{context.job_id}] (2/6) Starting Classification...")
    t_start = time.time()
    if not context.extracted_text:
        log.warning(f"[{context.job_id}] No text extracted from OCR. Skipping classification.")
        context.classification_result = {"doc_type": "ocr_failed", "confidence": 0.0}
        return
    try:
        classification_result = classifier_model.classify(
            text=context.extracted_text,
            file_name=context.file_name
        )
        context.classification_result = classification_result
        context.classification_time = time.time() - t_start
        log.info(
            f"[{context.job_id}] (2/6) Classification finished ({context.classification_time:.2f}s). Result: {classification_result}")
    except Exception as e:
        log.error(f"[{context.job_id}] Classification task failed: {e}", exc_info=True)
        context.classification_result = {"doc_type": "classification_failed", "confidence": 0.0}


def perform_llm_extraction(context: DocumentContext):
    doc_type = context.classification_result.get("doc_type", "unknown")
    extraction_task = get_llm_extraction_task(doc_type)
    if not extraction_task:
        log.info(f"[{context.job_id}] (3/6) No extraction task defined for doc_type '{doc_type}'. Skipping.")
        return
    log.info(f"[{context.job_id}] (3/6) Starting LLM Extraction for '{doc_type}'...")
    t_start = time.time()
    try:
        context.structured_data = extraction_task(context.extracted_text)
        context.extraction_time = time.time() - t_start
        log.info(f"[{context.job_id}] (3/6) LLM Extraction finished ({context.extraction_time:.2f}s).")
    except Exception as e:
        log.error(f"[{context.job_id}] LLM Extraction task failed: {e}", exc_info=True)
        context.structured_data = {"error": str(e)}


def perform_pii_and_summary(context: DocumentContext):
    log.info(f"[{context.job_id}] (4/6) Starting PII Masking...")
    try:
        context.pii_results = perform_pii_masking(context.extracted_text)
        log.info(f"[{context.job_id}] (4/6) PII Masking finished.")
    except Exception as e:
        log.error(f"[{context.job_id}] PII Masking task failed: {e}", exc_info=True)
    log.info(f"[{context.job_id}] (5/6) Starting Summarization...")
    try:
        context.summary = perform_summarization(context.extracted_text)
        log.info(f"[{context.job_id}] (5/6) Summarization finished.")
    except Exception as e:
        log.error(f"[{context.job_id}] Summarization task failed: {e}", exc_info=True)


def perform_indexing(context: DocumentContext):
    """6. ê²€ìƒ‰ ì—”ì§„ì— ì¸ë±ì‹±"""
    log.info(f"[{context.job_id}] (6/6) Starting Indexing...")
    try:
        # 1. MeiliSearch (Full-Text Search)
        meili_doc = {
            "id": context.job_id,  # ğŸš¨ [ìˆ˜ì • 8]: MeiliSearchì˜ PKëŠ” 'id'ë¡œ ì„¤ì • (client.pyì™€ ì¼ì¹˜)
            "document_id": context.job_id,  # document_idë„ ìœ ì§€
            "file_name": context.file_name,
            "doc_type": context.classification_result.get("doc_type", "unknown"),
            "content": context.extracted_text,
            "summary": context.summary,
            "created_at": int(context.start_time),
            **context.structured_data
        }
        # ğŸš¨ [ìˆ˜ì • 9]: client.pyì— ì •ì˜ëœ add_document í•¨ìˆ˜ ì‚¬ìš©
        search_client.add_document(meili_doc)
        log.info(f"[{context.job_id}] (6/6) MeiliSearch indexing finished.")

        # 2. Qdrant (Vector Search)
        # ğŸš¨ [ìˆ˜ì • 10]: client.pyì˜ add_vectorsì— ë§ê²Œ ë°ì´í„° í¬ë§·íŒ…
        # QdrantëŠ” í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ë°›ì§€ ì•Šê³ , ë²¡í„°ì™€ payloadë¥¼ ë°›ìŠµë‹ˆë‹¤.
        # [ì£¼ì˜] ì‹¤ì œë¡œëŠ” ì—¬ê¸°ì„œ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ë‹¨ê³„ê°€ í•„ìš”í•©ë‹ˆë‹¤.
        # ì§€ê¸ˆì€ ì„ë² ë”© ë‹¨ê³„ê°€ ì—†ìœ¼ë¯€ë¡œ, ì„ì‹œë¡œ Qdrant ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤.

        # vector_client.index_document( ... ) # <- ì´ í•¨ìˆ˜ëŠ” client.pyì— ì¡´ì¬í•˜ì§€ ì•ŠìŒ

        # [ì„ì‹œ ì¡°ì¹˜] Qdrant ì¸ë±ì‹±ì€ ì„ë² ë”© íŒŒì´í”„ë¼ì¸ì´ ì¶”ê°€ë  ë•Œê¹Œì§€ ë¹„í™œì„±í™”
        log.warning(f"[{context.job_id}] Qdrant indexing skipped: Embedding pipeline not implemented yet.")

        # [ì°¸ê³ ] ì‹¤ì œ êµ¬í˜„ ì‹œ:
        # points = [
        #     qdrant_models.PointStruct(
        #         id=str(uuid.uuid4()),
        #         vector=embedding_function(context.extracted_text), # [í•„ìˆ˜] ì„ë² ë”© í•¨ìˆ˜ í•„ìš”
        #         payload=meili_doc
        #     )
        # ]
        # vector_client.add_vectors(points)
        # log.info(f"[{context.job_id}] (6/6) Qdrant indexing finished.")

    except Exception as e:
        log.error(f"[{context.job_id}] Indexing task failed: {e}", exc_info=True)


def cleanup_file(file_path: str):
    try:
        if os.path.exists(file_path):
            os.remove(file_path)
            log.info(f"Cleaned up temp file: {file_path}")
    except Exception as e:
        log.error(f"Failed to cleanup file {file_path}: {e}")


# --- ë©”ì¸ Celery ì‘ì—… ---
@celery_app.task(name="process_document_pipeline")
def process_document_pipeline(
        job_id: str,
        file_path: str,
        file_name: str,
        file_mime_type: str
):
    log.info(f"[{job_id}] Celery task started for file: {file_name}")
    context = DocumentContext(
        job_id=job_id,
        file_path=file_path,
        file_name=file_name,
        file_mime_type=file_mime_type
    )
    try:
        perform_ocr(context)
        perform_classification(context)
        perform_llm_extraction(context)
        perform_pii_and_summary(context)
        perform_indexing(context)

        context.total_time = time.time() - context.start_time
        log.info(f"[{job_id}] Pipeline COMPLETED ({context.total_time:.2f}s).")
        cleanup_file(file_path)
        return json.loads(context.model_dump_json())

    except Exception as e:
        log.critical(f"[{job_id}] Unhandled error in pipeline: {e}", exc_info=True)
        cleanup_file(file_path)
        error_result = context.model_dump()
        error_result["pipeline_status"] = "FAILED"
        error_result["error"] = str(e)
        return error_result