# í•´ì»¤í†¤ ë‹¹ì¼ ì‹¤í–‰ ê°€ì´ë“œ ğŸƒâ€â™‚ï¸

## â° íƒ€ì„ë¼ì¸ (ì´ 2ì‹œê°„ ê¸°ì¤€)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  00:00 ~ 00:10 (10ë¶„)  â”‚  ë°ì´í„° í™•ì¸ ë° í™˜ê²½ ì¤€ë¹„      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  00:10 ~ 00:50 (40ë¶„)  â”‚  Training set OCR ì²˜ë¦¬         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  00:50 ~ 01:50 (60ë¶„)  â”‚  ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  01:50 ~ 02:10 (20ë¶„)  â”‚  Testing set ì˜ˆì¸¡ ë° ê²€ì¦      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  02:10 ~ 02:20 (10ë¶„)  â”‚  ê²°ê³¼ ì œì¶œ ë° ì—¬ìœ  ì‹œê°„        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ ì‚¬ì „ ì¤€ë¹„ (í•´ì»¤í†¤ ì‹œì‘ ì „)

### âš ï¸ ì²˜ìŒ í™˜ê²½ ì„¤ì •í•˜ëŠ” ê²½ìš°

**venvê°€ ì—†ê±°ë‚˜ ìƒˆ ì»´í“¨í„°ë¼ë©´:**

```bash
# Python 3.11 ì„¤ì¹˜ í™•ì¸
python3.11 --version

# ì—†ìœ¼ë©´ ì„¤ì¹˜
brew install python@3.11

# ê°€ìƒí™˜ê²½ ìƒì„± + íŒ¨í‚¤ì§€ ì„¤ì¹˜
python3.11 -m venv venv
source venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt

# 5-10ë¶„ ëŒ€ê¸°...
```

**ìì„¸í•œ ì„¤ëª…ì€ â†’ [README.md](./README.md)ì˜ "0ë‹¨ê³„" ì°¸ê³ **

---

### âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

```bash
# 1. ê°€ìƒí™˜ê²½ í…ŒìŠ¤íŠ¸
source venv/bin/activate
python --version  # Python 3.11.x í™•ì¸

# 2. íŒ¨í‚¤ì§€ í™•ì¸
python -c "from src.ocr_module import OCRModule; print('âœ“ OCR OK')"
python -c "from src.classification_module import DocumentClassifier; print('âœ“ Classifier OK')"
python -c "from src.extraction_module import DataExtractor; print('âœ“ Extractor OK')"

# 3. í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ì‹¤í–‰
python src/batch_ocr.py --input test_samples --output test_ocr.json
# â†’ ì—ëŸ¬ ì—†ì´ ì™„ë£Œë˜ë©´ OK!

# 4. ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p outputs
mkdir -p models
```

### ğŸ“‹ ì¤€ë¹„ë¬¼

- [ ] ë…¸íŠ¸ë¶ ì¶©ì „ ì™„ë£Œ
- [ ] ì¸í„°ë„· ì—°ê²° í™•ì¸
- [ ] ì´ ê°€ì´ë“œ ì¶œë ¥ ë˜ëŠ” ë¶ë§ˆí¬
- [ ] ëª…ë ¹ì–´ í…œí”Œë¦¿ ë³µì‚¬ (ì•„ë˜ì— ìˆìŒ)
- [ ] íŒ€ì›ê³¼ ì—­í•  ë¶„ë‹´ ì™„ë£Œ

---

## ğŸ¬ í•´ì»¤í†¤ ì‹œì‘! (Step by Step)

### Step 0: ë°ì´í„° ìˆ˜ì‹  ë° í™•ì¸ (10ë¶„)

#### ë°›ëŠ” ë°ì´í„°

```
provided_data/
â”œâ”€â”€ training_set/
â”‚   â”œâ”€â”€ documents/        # 500-1000ê°œ ë¬¸ì„œ
â”‚   â”‚   â”œâ”€â”€ doc001.jpg
â”‚   â”‚   â”œâ”€â”€ doc002.pdf
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ labels.csv       # ì •ë‹µ ë ˆì´ë¸”
â”‚
â””â”€â”€ testing_set/
    â””â”€â”€ documents/        # 100ê°œ ë¬¸ì„œ (ë ˆì´ë¸” ì—†ìŒ!)
        â”œâ”€â”€ test001.jpg
        â””â”€â”€ ...
```

#### labels.csv êµ¬ì¡° í™•ì¸

```bash
# labels.csv ì—´ê¸°
head -n 5 training_set/labels.csv
```

**ì˜ˆìƒ ì¶œë ¥ 1 (Invoice/Receipt)**:
```csv
filename,doc_type,vendor,amount,date
doc001.jpg,invoice,ABC Company,1500.75,2025-01-01
doc002.png,receipt,SuperMart,89.90,2025-01-02
```

**ì˜ˆìƒ ì¶œë ¥ 2 (Resume)** âš ï¸:
```csv
filename,doc_type,name,experience,education
doc001.pdf,resume,John Smith,5 years,MIT
```

**ì¤‘ìš”**: ì»¬ëŸ¼ì„ í™•ì¸í•˜ì„¸ìš”!
- `vendor, amount, date` â†’ Invoice/Receipt (ì˜ˆìƒëŒ€ë¡œ)
- `name, experience, education` â†’ Resume (ì˜ˆìƒ ë°–!)
- ë‹¤ë¥¸ í•„ë“œë©´ â†’ ê¸´ê¸‰ ìˆ˜ì • í•„ìš” (ë‚˜ì¤‘ì— ì„¤ëª…)

#### ìƒ˜í”Œ í™•ì¸

```bash
# ë¬¸ì„œ ê°œìˆ˜ í™•ì¸
ls training_set/documents | wc -l   # 500-1000ê°œ?
ls testing_set/documents | wc -l    # 100ê°œ?

# ìƒ˜í”Œ 1-2ê°œ ëˆˆìœ¼ë¡œ í™•ì¸
open training_set/documents/doc001.jpg  # Mac
# ë˜ëŠ”
xdg-open training_set/documents/doc001.jpg  # Linux
```

**í™•ì¸ì‚¬í•­**:
- [ ] ì´ë¯¸ì§€ê°€ íë¦¿í•˜ì§€ ì•Šì€ê°€?
- [ ] íšŒì „ë˜ì–´ ìˆì§€ ì•Šì€ê°€?
- [ ] PDFê°€ ìŠ¤ìº”ë³¸ì¸ê°€ í…ìŠ¤íŠ¸ PDFì¸ê°€?

---

### Step 1: Training Set OCR (40ë¶„)

**ëª©í‘œ**: training_setì˜ ëª¨ë“  ë¬¸ì„œë¥¼ OCR ì²˜ë¦¬

#### ëª…ë ¹ì–´ (ë³µì‚¬ í›„ ì‹¤í–‰)

```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™” (í„°ë¯¸ë„ ìƒˆë¡œ ì—´ì—ˆìœ¼ë©´)
source venv/bin/activate

# OCR ì‹¤í–‰ (500ê°œ ê¸°ì¤€ 30-40ë¶„ ì†Œìš”)
python src/batch_ocr.py \
  --input training_set/documents \
  --output outputs/training_ocr.json

# ì™„ë£Œë˜ë©´ íŒŒì¼ í¬ê¸° í™•ì¸
ls -lh outputs/training_ocr.json
# â†’ 1-5MB ì •ë„ë©´ ì •ìƒ
```

#### ì§„í–‰ ì¤‘ ì¶œë ¥

```
Scanning files...
Found 500 documents

OCR Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [35:23<00:00,  4.23s/doc]

âœ“ Processed: 497 documents
âœ— Errors: 3 documents

Statistics:
  Total files: 500
  Successful: 497
  Failed: 3
  Average confidence: 94.2%
  Total time: 35m 23s

Saved to: outputs/training_ocr.json
```

#### ì—ëŸ¬ ë°œìƒ ì‹œ

**ì—ëŸ¬: "Out of memory"**
```bash
# íŒŒì¼ì„ 2ê°œë¡œ ë‚˜ëˆ ì„œ ì²˜ë¦¬
mkdir training_set/documents_part1
mkdir training_set/documents_part2
# ... íŒŒì¼ ì ˆë°˜ì”© ì˜®ê¸°ê¸° ...

# ê°ê° OCR
python src/batch_ocr.py --input training_set/documents_part1 --output outputs/ocr1.json
python src/batch_ocr.py --input training_set/documents_part2 --output outputs/ocr2.json

# JSON í•©ì¹˜ê¸°
python -c "
import json
with open('outputs/ocr1.json') as f1, open('outputs/ocr2.json') as f2:
    data1 = json.load(f1)
    data2 = json.load(f2)
    combined = data1 + data2
    with open('outputs/training_ocr.json', 'w') as out:
        json.dump(combined, out)
"
```

#### OCR ì¤‘ í•  ì¼

OCRì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤. ê¸°ë‹¤ë¦¬ëŠ” ë™ì•ˆ:

1. **Testing set ìƒ˜í”Œ í™•ì¸**: ì–´ë–¤ ë¬¸ì„œë“¤ì¸ì§€ ë¯¸ë¦¬ ë³´ê¸°
2. **LLM íŒ€ê³¼ ì†Œí†µ**: JSON í˜•ì‹ ìµœì¢… í™•ì¸
3. **labels.csv ì¬í™•ì¸**: ì»¬ëŸ¼ ì´ë¦„ì´ ì´ìƒí•˜ë©´ ëŒ€ì‘ ì¤€ë¹„
4. **íœ´ì‹**: ì»¤í”¼ í•œì” â˜•

---

### Step 2: ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ (60ë¶„)

**ëª©í‘œ**: DistilBERT ëª¨ë¸ì„ training_setìœ¼ë¡œ í•™ìŠµ

#### ëª…ë ¹ì–´

```bash
# í•™ìŠµ ì‹œì‘ (500-1000ê°œ ê¸°ì¤€ 30-60ë¶„)
python src/train_classifier.py \
  --labels training_set/labels.csv \
  --ocr outputs/training_ocr.json \
  --output models/classifier
```

#### ì§„í–‰ ì¤‘ ì¶œë ¥

```
============================================================
Training Classification Model
============================================================

Step 1: Loading data...
Loaded 500 labels from CSV
Loaded 500 OCR results

Step 2: Preparing training data...
Prepared 497 training samples
Skipped 3 samples due to errors

Step 3: Creating dataset...
Dataset created with 497 samples

Step 4: Tokenizing text...
Tokenization complete!

Step 5: Initializing model...
Model initialized for 5 classes

Step 6: Configuring training...
Training configuration set

Step 7: Starting training...
This may take 30-60 minutes...

Epoch 1/3:
  Loss: 0.892 | Accuracy: 72.3%
Epoch 2/3:
  Loss: 0.234 | Accuracy: 91.5%
Epoch 3/3:
  Loss: 0.089 | Accuracy: 96.8%

Training complete!

Step 8: Saving model...
Model saved to models/classifier

============================================================
Training Complete!
============================================================
```

#### í•™ìŠµ ì¤‘ ëª¨ë‹ˆí„°ë§

**ì¢‹ì€ ì‹ í˜¸** âœ…:
- Lossê°€ ì ì  ê°ì†Œ (0.8 â†’ 0.2 â†’ 0.08)
- Accuracyê°€ ì ì  ì¦ê°€ (70% â†’ 90% â†’ 96%)
- Epoch 3ì—ì„œ Accuracy > 90%

**ë‚˜ìœ ì‹ í˜¸** âš ï¸:
- Lossê°€ ì¦ê°€í•˜ê±°ë‚˜ ë³€ë™ ì‹¬í•¨
- Accuracyê°€ 70% ë¯¸ë§Œì—ì„œ ë©ˆì¶¤
- "NaN" ì—ëŸ¬ ë°œìƒ

**ë‚˜ìœ ì‹ í˜¸ ë°œìƒ ì‹œ**:
```bash
# í•™ìŠµ ì¤‘ë‹¨ (Ctrl+C)
# learning_rate ë‚®ì¶”ê¸°
# train_classifier.py ì—´ì–´ì„œ ìˆ˜ì •:
# learning_rate=2e-5 â†’ learning_rate=5e-6
# ë‹¤ì‹œ ì‹¤í–‰
```

#### í•™ìŠµ ì¤‘ í•  ì¼

1. **Testing set OCR (ì„ íƒ)**: ë¯¸ë¦¬ í•´ë‘ë©´ ì‹œê°„ ì ˆì•½
   ```bash
   # ìƒˆ í„°ë¯¸ë„ ì—´ì–´ì„œ
   python src/batch_ocr.py \
     --input testing_set/documents \
     --output outputs/testing_ocr.json
   ```

2. **ëª¨ë¸ ê²€ì¦ ì¤€ë¹„**: ìƒ˜í”Œ 1-2ê°œë¡œ í…ŒìŠ¤íŠ¸í•  ì¤€ë¹„

---

### Step 3: Testing Set ì˜ˆì¸¡ (20ë¶„)

**ëª©í‘œ**: í•™ìŠµëœ ëª¨ë¸ë¡œ testing_set ì˜ˆì¸¡ â†’ predictions.json ìƒì„±

#### ëª…ë ¹ì–´

```bash
# ì˜ˆì¸¡ ì‹¤í–‰ (100ê°œ ê¸°ì¤€ 10-20ë¶„)
python src/predict.py \
  --input testing_set/documents \
  --classifier models/classifier \
  --labels training_set/labels.csv \
  --output predictions.json
```

#### ì§„í–‰ ì¤‘ ì¶œë ¥

```
============================================================
Batch Prediction for Testing Set
============================================================

Initializing modules...
Loading NER model...
Configuring extractor from training_set/labels.csv...
  invoice: ['vendor', 'amount', 'date']
  receipt: ['store', 'total', 'date']
Extraction configured for 2 document types
Modules ready!

Scanning files...
Found 100 documents

Processing documents...
Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [12:34<00:00,  7.54s/doc]

Processed: 98 documents
Errors: 2 documents

Saved to: predictions.json
Errors saved to: predictions_errors.json
```

---

### Step 4: ê²°ê³¼ ê²€ì¦ (10ë¶„)

#### predictions.json ìƒ˜í”Œ í™•ì¸

```bash
# ì²˜ìŒ 2ê°œ ë¬¸ì„œ í™•ì¸
python -c "
import json
with open('predictions.json') as f:
    data = json.load(f)
    for item in data[:2]:
        print(json.dumps(item, indent=2))
"
```

**ì˜ˆìƒ ì¶œë ¥**:
```json
{
  "filename": "test001.jpg",
  "full_text_ocr": "Commercial Invoice\nCompany: ABC...",
  "ocr_confidence": 0.97,
  "classification": {
    "doc_type": "invoice",
    "confidence": 0.96
  },
  "extracted_data": {
    "vendor": "ABC Exports Ltd",
    "date": "2030-09-30",
    "total_amount": 13000.0,
    "currency": "USD"
  }
}
```

#### ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] `filename`ì´ ì˜¬ë°”ë¥¸ê°€?
- [ ] `full_text_ocr`ì— í…ìŠ¤íŠ¸ê°€ ìˆëŠ”ê°€?
- [ ] `classification.doc_type`ì´ í•©ë¦¬ì ì¸ê°€?
- [ ] `classification.confidence`ê°€ 80% ì´ìƒì¸ê°€?
- [ ] `extracted_data`ì— ê°’ì´ ìˆëŠ”ê°€? (invoice/receiptì¸ ê²½ìš°)

#### í†µê³„ í™•ì¸

```bash
python -c "
import json
with open('predictions.json') as f:
    data = json.load(f)
    
print(f'Total predictions: {len(data)}')

# ë¬¸ì„œ íƒ€ì… ë¶„í¬
from collections import Counter
types = [d['classification']['doc_type'] for d in data]
print('\nDocument types:')
for dtype, count in Counter(types).items():
    print(f'  {dtype}: {count}')

# í‰ê·  ì‹ ë¢°ë„
confidences = [d['classification']['confidence'] for d in data]
avg_conf = sum(confidences) / len(confidences)
print(f'\nAverage confidence: {avg_conf:.2%}')
"
```

**ì˜ˆìƒ ì¶œë ¥**:
```
Total predictions: 98

Document types:
  invoice: 45
  receipt: 32
  resume: 15
  report: 6

Average confidence: 93.4%
```

---

### Step 5: ì œì¶œ ì¤€ë¹„ (10ë¶„)

#### ìµœì¢… íŒŒì¼ í™•ì¸

```bash
# íŒŒì¼ ì¡´ì¬ ë° í¬ê¸° í™•ì¸
ls -lh predictions.json
# â†’ 100KB ~ 5MB ì‚¬ì´ë©´ ì •ìƒ

# JSON í˜•ì‹ ê²€ì¦
python -m json.tool predictions.json > /dev/null && echo "âœ“ Valid JSON"
```

#### ì œì¶œ ì „ ìµœì¢… ì²´í¬

```bash
# ì˜ˆì¸¡ ê°œìˆ˜ = testing_set ê°œìˆ˜?
python -c "import json; print(len(json.load(open('predictions.json'))))"
# vs
ls testing_set/documents | wc -l

# ë‘ ìˆ«ìê°€ ë¹„ìŠ·í•´ì•¼ í•¨ (ì—ëŸ¬ 2-3ê°œëŠ” OK)
```

#### ë°±ì—… ìƒì„±

```bash
# ë§Œì•½ì„ ìœ„í•´ ë°±ì—…
cp predictions.json predictions_backup_$(date +%H%M).json
```

---

## ğŸš¨ ê¸´ê¸‰ ìƒí™© ëŒ€ì‘

### ìƒí™© 1: labels.csvì˜ ì»¬ëŸ¼ì´ ì˜ˆìƒê³¼ ë‹¤ë¦„

**ì˜ˆ**: `vendor, amount, date` ëŒ€ì‹  `name, salary, position`

#### í•´ê²°ì±…

**ìë™ ëŒ€ì‘** (ì¶”ì¶œ ëª¨ë“ˆì´ ìë™ìœ¼ë¡œ ì²˜ë¦¬):
```bash
# ê·¸ëƒ¥ ì‹¤í–‰í•˜ë©´ ìë™ìœ¼ë¡œ í•„ë“œë¥¼ ì¸ì‹!
python src/predict.py \
  --input testing_set/documents \
  --classifier models/classifier \
  --labels training_set/labels.csv \
  --output predictions.json
```

extraction_module.pyì˜ `configure_from_labels()`ê°€:
1. labels.csv ì—´ê¸°
2. ì»¬ëŸ¼ í™•ì¸
3. ìë™ìœ¼ë¡œ ì¶”ì¶œ ì„¤ì • ì¡°ì •

**ìˆ˜ë™ í™•ì¸**:
```bash
# ì¶”ì¶œ ì„¤ì • í™•ì¸
python -c "
from src.extraction_module import DataExtractor
extractor = DataExtractor()
extractor.configure_from_labels('training_set/labels.csv')
print(extractor.extraction_config)
"
# â†’ {'resume': ['name', 'salary', 'position']} ì¶œë ¥ë˜ë©´ OK
```

---

### ìƒí™© 2: OCR ì‹ ë¢°ë„ê°€ ë„ˆë¬´ ë‚®ìŒ (< 70%)

#### ì›ì¸

- ì´ë¯¸ì§€ í’ˆì§ˆ ë¬¸ì œ
- PDFê°€ ìŠ¤ìº”ë³¸ì´ ì•„ë‹ˆë¼ í…ìŠ¤íŠ¸ PDF

#### í•´ê²°ì±…

```bash
# ì‹ ë¢°ë„ ë‚®ì€ íŒŒì¼ ì°¾ê¸°
python -c "
import json
with open('outputs/training_ocr.json') as f:
    data = json.load(f)
    low_conf = [(item['filename'], item['confidence']) 
                for item in data if item['confidence'] < 0.7]
    for fname, conf in low_conf[:10]:
        print(f'{fname}: {conf:.2%}')
"

# í•´ë‹¹ íŒŒì¼ë“¤ ìˆ˜ë™ í™•ì¸
# â†’ ì •ë§ íë¦¿í•˜ë©´ ì–´ì©” ìˆ˜ ì—†ìŒ
# â†’ íšŒì „ë˜ì–´ ìˆìœ¼ë©´ ì´ë¯¸ì§€ íšŒì „ í›„ ì¬ì‹¤í–‰
```

---

### ìƒí™© 3: ë¶„ë¥˜ ì •í™•ë„ê°€ ë‚®ìŒ (< 85%)

#### í•´ê²°ì±… 1: Epoch ì¦ê°€

```python
# src/train_classifier.py ìˆ˜ì •
# 59ë²ˆ ì¤„:
num_train_epochs=3  â†’ num_train_epochs=5
```

#### í•´ê²°ì±… 2: Learning rate ê°ì†Œ

```python
# 61ë²ˆ ì¤„:
learning_rate=2e-5  â†’ learning_rate=5e-6
```

---

### ìƒí™© 4: ì¶”ì¶œ ê²°ê³¼ê°€ ëŒ€ë¶€ë¶„ None

#### ì›ì¸

- NER ëª¨ë¸ì´ ì—”í‹°í‹°ë¥¼ ëª» ì°¾ìŒ
- ì •ê·œì‹ íŒ¨í„´ì´ ë§ì§€ ì•ŠìŒ

#### í•´ê²°ì±…

```bash
# ìƒ˜í”Œ 1ê°œë¡œ ë””ë²„ê¹…
python -c "
from src.extraction_module import DataExtractor
extractor = DataExtractor()

text = '''
Invoice
Company: ABC Ltd
Total: 1500 USD
Date: 2025-01-01
'''

entities = extractor._run_ner(text)
print('Entities:', entities)

patterns = extractor._extract_patterns(text)
print('Patterns:', patterns)
"

# ì—”í‹°í‹°ì™€ íŒ¨í„´ì´ ì œëŒ€ë¡œ ì¶”ì¶œë˜ëŠ”ì§€ í™•ì¸
# ì•ˆ ë˜ë©´ extraction_module.py ìˆ˜ì • í•„ìš”
```

---

## ğŸ¯ ì„±ê³µì„ ìœ„í•œ íŒ

### ì‹œê°„ ê´€ë¦¬

```
ìš°ì„ ìˆœìœ„:
1. OCR (í•„ìˆ˜!) â†’ 40ë¶„
2. í•™ìŠµ (í•„ìˆ˜!) â†’ 60ë¶„
3. ì˜ˆì¸¡ (í•„ìˆ˜!) â†’ 20ë¶„
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ì´ 120ë¶„ (2ì‹œê°„)

ì—¬ìœ  ì‹œê°„: 20ë¶„
```

**ë§Œì•½ ì‹œê°„ì´ ë¶€ì¡±í•˜ë©´**:
- Epoch ê°ì†Œ: 3 â†’ 2 (10-20ë¶„ ì ˆì•½)
- Testing set OCR ë³‘ë ¬ ì‹¤í–‰ (10ë¶„ ì ˆì•½)

### íŒ€ ì—­í•  ë¶„ë‹´

**ë‹´ë‹¹ì 1 (AI ë‹´ë‹¹)**:
- OCR ì‹¤í–‰ ë° ëª¨ë‹ˆí„°ë§
- ëª¨ë¸ í•™ìŠµ ë° ê²€ì¦
- ì—ëŸ¬ ë°œìƒ ì‹œ ë””ë²„ê¹…

**ë‹´ë‹¹ì 2 (ë°ì´í„° ë‹´ë‹¹)**:
- labels.csv ë¶„ì„
- ìƒ˜í”Œ ë¬¸ì„œ í™•ì¸
- ê²°ê³¼ ê²€ì¦ ë° í†µê³„

**ë‹´ë‹¹ì 3 (í˜‘ì—… ë‹´ë‹¹)**:
- LLM íŒ€ê³¼ ì†Œí†µ
- JSON í˜•ì‹ í™•ì¸
- ë°±ì—… ë° ì œì¶œ ì¤€ë¹„

### ì»¤ë®¤ë‹ˆì¼€ì´ì…˜

**LLM íŒ€ì—ê²Œ ë¯¸ë¦¬ ì•Œë ¤ì£¼ê¸°**:
```json
{
  "filename": "ë¬¸ì„œ íŒŒì¼ëª…",
  "full_text_ocr": "ì „ì²´ OCR í…ìŠ¤íŠ¸ (ìš”ì•½/PII ê²€ì¶œì— ì‚¬ìš©)",
  "classification": {
    "doc_type": "ë¬¸ì„œ íƒ€ì… (invoice/receipt/...)",
    "confidence": "ë¶„ë¥˜ ì‹ ë¢°ë„"
  },
  "extracted_data": {
    "vendor": "êµ¬ì¡°í™”ëœ ì¶”ì¶œ ê²°ê³¼ (invoice/receiptë§Œ)",
    "amount": 1500.75,
    "date": "2025-01-01"
  }
}
```

**ì§ˆë¬¸ ì˜ˆìƒ**:
- Q: "full_text_ocrì´ ì™œ ì´ë ‡ê²Œ ê¸¸ì–´ìš”?"
  - A: "ì›ë³¸ í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œì…ë‹ˆë‹¤. ìš”ì•½ì€ LLMì´ í•´ì£¼ì„¸ìš”."
  
- Q: "extracted_dataê°€ ë¹„ì–´ìˆëŠ” ë¬¸ì„œê°€ ìˆì–´ìš”"
  - A: "resume, report, contractëŠ” ì¶”ì¶œ ì•ˆ í•©ë‹ˆë‹¤. full_text_ocr ì‚¬ìš©í•´ì£¼ì„¸ìš”."

---

## ğŸŠ ì™„ë£Œ!

### ì œì¶œ ì „ ìµœì¢… ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] predictions.json íŒŒì¼ ì¡´ì¬
- [ ] JSON í˜•ì‹ ê²€ì¦ ì™„ë£Œ
- [ ] ì˜ˆì¸¡ ê°œìˆ˜ = testing_set ê°œìˆ˜ (Â±5ê°œ ì´ë‚´)
- [ ] ìƒ˜í”Œ 2-3ê°œ ìœ¡ì•ˆ í™•ì¸
- [ ] í‰ê·  ì‹ ë¢°ë„ > 85%
- [ ] ë°±ì—… íŒŒì¼ ìƒì„± ì™„ë£Œ
- [ ] LLM íŒ€ì—ê²Œ í˜•ì‹ ì „ë‹¬ ì™„ë£Œ

### ì œì¶œ í›„

1. **ë¡œê·¸ ì €ì¥**: ëª¨ë“  í„°ë¯¸ë„ ì¶œë ¥ ë³µì‚¬í•´ì„œ ì €ì¥
2. **í†µê³„ ê¸°ë¡**: ì •í™•ë„, ì²˜ë¦¬ ì‹œê°„ ë“± ê¸°ë¡
3. **íŒ€ í”¼ë“œë°±**: ì–´ë–¤ ë¶€ë¶„ì´ ì–´ë ¤ì› ëŠ”ì§€ ê³µìœ 

---

## ğŸ“ ì¶”ê°€ ë„ì›€ì´ í•„ìš”í•˜ë©´

- **ARCHITECTURE.md**: í•¨ìˆ˜ë³„ ìƒì„¸ ì„¤ëª…
- **README.md**: í”„ë¡œì íŠ¸ ì „ì²´ ê°œìš”
- **ê¸´ê¸‰_ìˆ˜ì •_ê°€ì´ë“œ.md**: ì½”ë“œ ìˆ˜ì •ì´ í•„ìš”í•œ ê²½ìš°

---

**í™”ì´íŒ…! ğŸš€ ì„±ê³µì„ ê¸°ì›í•©ë‹ˆë‹¤!**

