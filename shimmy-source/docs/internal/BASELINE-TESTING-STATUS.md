# MoE Baseline Testing Progress
<<<<<<< HEAD
<<<<<<< HEAD
**Date**: October 8, 2025
=======
**Date**: October 8, 2025  
>>>>>>> main
=======
**Date**: October 8, 2025  
>>>>>>> main
**Status**: RUNNING controlled A/B baseline tests

---

## What We're Doing NOW

### Immediate Actions (In Progress)
1. **Running GPT-OSS A/B Baseline** ⏳ IN PROGRESS
   - Test 1: Baseline (NO --cpu-moe) on port 11436
   - Test 2: Offload (WITH --cpu-moe) on port 11437
   - Each: 4 prompts × 3 runs = 12 measurements per configuration
   - Measuring: VRAM (nvidia-smi), tokens, time, TPS, TTFT

2. **Next: Phi-3.5-MoE A/B Baseline** ⏳ QUEUED
   - Same methodology
   - Expected runtime: ~20-30 minutes

3. **Next: DeepSeek A/B Baseline** ⏳ QUEUED
   - Same methodology
   - Expected runtime: ~20-30 minutes

### What This Fixes
**GPT-5's Top Critiques**:
- ✅ **Real baselines** (not estimates): With/without --cpu-moe on same hardware
- ✅ **Statistical validity**: N=3 runs (can calculate mean ± σ)
- ✅ **Accurate VRAM measurements**: nvidia-smi process-specific VRAM
- ⚠️ **Token counting**: Still using SSE chunk count (need tokenizer fix later)
- ⚠️ **TTFT**: Still estimated at 10% (need per-token timestamps later)

---

## Methodology

### Test Design
```
┌─────────────────────────────────────┐
│ BASELINE (port 11436)               │
│ shimmy serve (NO --cpu-moe)         │
│ - Measure VRAM after model load     │
│ - Run 4 prompts × 3 times           │
│ - Record: tokens, time, TPS, TTFT   │
└─────────────────────────────────────┘
         ↓ Stop server, sleep 5s
┌─────────────────────────────────────┐
│ OFFLOAD (port 11437)                │
│ shimmy serve (WITH --cpu-moe)       │
│ - Measure VRAM after model load     │
│ - Run same 4 prompts × 3 times      │
│ - Record: tokens, time, TPS, TTFT   │
└─────────────────────────────────────┘
         ↓ Compare results
┌─────────────────────────────────────┐
│ SUMMARY                             │
│ - VRAM reduction %                  │
│ - TPS comparison (mean ± σ)         │
│ - TTFT comparison (mean ± σ)        │
│ - Performance overhead %            │
└─────────────────────────────────────┘
```

### Test Prompts (4 lengths)
1. **Short (7 tokens)**: "Write a haiku about AI"
2. **Medium (6 tokens)**: "Explain quantum computing in simple terms"
3. **Long (10 tokens)**: "Write a Python function to calculate fibonacci numbers recursively"
4. **Very Long (27 tokens)**: "Write a detailed technical explanation of how gradient descent optimization works in machine learning"

### Parameters
- `max_tokens`: 100
- `temperature`: 0.3
- `stream`: true (SSE mode)
- `N`: 3 runs per prompt per configuration

---

## Expected Timeline

| Task | Duration | Status |
|------|----------|--------|
| GPT-OSS baseline | ~15-20 min | ⏳ Running |
| Phi-3.5-MoE baseline | ~20-30 min | ⏳ Queued |
| DeepSeek baseline | ~20-30 min | ⏳ Queued |
| **Total** | **~60-80 min** | |

---

## What Happens After

### Immediate (Tonight)
1. **Extract mean ± σ** from N=3 runs
2. **Update MOE-TECHNICAL-VALIDATION.md** with real baseline data
3. **Create comparison tables** (baseline vs offload)
4. **Calculate VRAM reduction %** (actual, not estimated)
5. **Document performance overhead** (TPS/TTFT impact of offloading)

### Medium Priority (This Week)
6. **Add SHA256 checksums** for all model files
7. **Fix token counting** (use model tokenizer, not SSE chunk count)
8. **Add per-token timestamps** (for real TTFT, not 10% estimate)
9. **Create 3 performance plots** (VRAM, TTFT, TPS)

### Low Priority (Future)
10. **Objective quality metrics** (embedding similarity, pass@k)
11. **Test on other hardware** (A100, H100, consumer GPUs)
12. **Test other quantizations** (Q4, Q5, Q8)

---

## Output Files

### Generated by baseline-ab-testing.sh
```
baseline-ab-gpt-oss-20b-YYYYMMDD-HHMMSS.log    # Full results
server-11436.log                                # Baseline server logs
server-11437.log                                # Offload server logs
```

### Will Update
```
docs/MOE-TECHNICAL-VALIDATION.md                # Insert real baseline data
docs/benchmark-evidence/                        # Copy baseline logs here
```

---

## Current Status

<<<<<<< HEAD
<<<<<<< HEAD
**Running**: GPT-OSS A/B baseline test
**Script**: `/home/ubuntu/shimmy/scripts/baseline-ab-testing.sh`
**Output**: Will be in `baseline-ab-gpt-oss-20b-*.log`
=======
**Running**: GPT-OSS A/B baseline test  
**Script**: `/home/ubuntu/shimmy/scripts/baseline-ab-testing.sh`  
**Output**: Will be in `baseline-ab-gpt-oss-20b-*.log`  
>>>>>>> main
=======
**Running**: GPT-OSS A/B baseline test  
**Script**: `/home/ubuntu/shimmy/scripts/baseline-ab-testing.sh`  
**Output**: Will be in `baseline-ab-gpt-oss-20b-*.log`  
>>>>>>> main
**ETA**: ~15-20 minutes

**Next**: Monitor progress, then queue Phi-3.5-MoE and DeepSeek

---

*Last updated: October 8, 2025 17:34 UTC*
